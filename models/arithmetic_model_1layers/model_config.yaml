act_fn: gelu
attention_dir: causal
attn_only: true
d_head: 64
d_model: 768
d_vocab: 14
n_ctx: 2048
n_heads: 12
n_layers: 1
normalization_type: null
positional_embedding_type: shortformer
seed: 398
tokenizer_name: null
use_attn_result: true
